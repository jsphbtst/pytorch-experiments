{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "huggingface_token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/config.json\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/generation_config.json\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/generation_config_for_text_generation.json\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/merges.txt\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/model.safetensors\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/pytorch_model.bin\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/rust_model.ot\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/tf_model.h5\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/tokenizer.json\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/tokenizer_config.json\n",
      "/Users/josephbautista/.cache/huggingface/hub/models--openai-community--openai-gpt/snapshots/1e0d4f3028acbffb47fe933cea64619c5ec1a002/vocab.json\n"
     ]
    }
   ],
   "source": [
    "model_id = \"openai-community/openai-gpt\"\n",
    "filenames = [\"config.json\", \"generation_config.json\", \"generation_config_for_text_generation.json\", \"merges.txt\", \"model.safetensors\", \"pytorch_model.bin\", \"rust_model.ot\", \"tf_model.h5\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"]\n",
    "\n",
    "for filename in filenames:\n",
    "  downloaded_model_path = hf_hub_download(\n",
    "    repo_id=model_id,\n",
    "    filename=filename,\n",
    "    token=huggingface_token\n",
    "  )\n",
    "\n",
    "  print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipeline = pipeline(\"text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Is this news headline positive or negative? 'Scholz Says EU Will Hit Back on Trump Tariffs But Deal Is Better'f he says he doesn't care if we get caught'if that is true. \\n'what about the press? what did they come up with at the end of the play? there are so many details. and how do it make sense that the first player to get the ball, a rich black named james hendry, was the first to hit a single shot from the pocket?'\\n'it's probably the same person who got the ball'berry says.'or was that an unconfirmed report '. \\n'but how can they go so bloody deep when you think there are millions of dollars in the bank?'\\n'i don't understand, did they know who the victim was?'\\n'i'm not sure'berry says.'so long as the killer's dead - maybe the bastard's not'in'here. who's dead?'\\n'dunno. but i've got word out on the press to be on their guard. \\n'yeah? the feds?'\\n'yeh?'\\n'well, i can do it too from the news.'\\n'what?'\\n'i can do it'\\n'what?'\\n'i know i can'get the money and get it to you, i can'get it'and it 'll do'him'for me.'\\n'you don't know'\\n'you 'll see,'he says,'if i make it a personal challenge.'\\n talbot's office hangs over the main hall of the talbot home. \\n'i won't be long.'\\n berry knocks twice and hears the muffled sounds of a phone playing. \\n'hello?'\\n'berry. in the office.'\\n berry opens the door. \\n there's a small, grey and black pile on the floor with a pile of dirty glasses and a stack of papers. \\n'berry? where are you?'\\n'up here.'\\n berry looks up at mr talbot's head above the desk. \\n'who's there?'talbot says. \\n'billy berry.'\\n talbot says'aye - aye, how's y'talbot?'but he doesn't sit up. \\n'i'm good.'\\n'well, why 'd you come here, if not for me?'\\n'i need the money.'\\n'jesus'fuck mate\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"Is this news headline positive or negative? 'Scholz Says EU Will Hit Back on Trump Tariffs But Deal Is Better'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
