{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "device mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "\n",
    "print(torch.backends.mps.is_available())  # Should print True\n",
    "print(torch.backends.mps.is_built())      # Should print True\n",
    "\n",
    "device = torch.device('mps')\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "nietzsche_corpus = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(nietzsche_corpus)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(nietzsche_corpus), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLSTM(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=3):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_size=embedding_dim,\n",
    "      hidden_size=hidden_size,\n",
    "      num_layers=num_layers,\n",
    "      dropout=0.2,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, hidden=None):\n",
    "    # idx shape: (batch, sequence)\n",
    "    embeddings = self.embedding(idx)  # (batch, sequence, embedding_dim)\n",
    "    output, hidden = self.lstm(embeddings, hidden)\n",
    "    logits = self.fc(output)  # (batch, sequence, vocab_size)\n",
    "    return logits, hidden\n",
    "\n",
    "model = CharacterLSTM(vocab_size=vocab_size, embedding_dim=384, hidden_size=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/5000], Loss: 1.3368\n",
      "Epoch [1000/5000], Loss: 1.2057\n",
      "Epoch [1500/5000], Loss: 1.1028\n",
      "Epoch [2000/5000], Loss: 1.0242\n",
      "Epoch [2500/5000], Loss: 1.0004\n",
      "Epoch [3000/5000], Loss: 0.9186\n",
      "Epoch [3500/5000], Loss: 0.9233\n",
      "Epoch [4000/5000], Loss: 0.9225\n",
      "Epoch [4500/5000], Loss: 0.8820\n",
      "Epoch [5000/5000], Loss: 0.8799\n"
     ]
    }
   ],
   "source": [
    "block_size = 256  # context length\n",
    "batch_size = 64   # batch size for training\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "\n",
    "  X, y = get_batch(train_data, block_size, batch_size)\n",
    "  X, y = X.to(model.embedding.weight.device), y.to(model.embedding.weight.device)\n",
    "\n",
    "  # Forward pass\n",
    "  optimizer.zero_grad()\n",
    "  logits, _ = model(X)\n",
    "\n",
    "  # Reshape for loss calculation\n",
    "  B, T, C = logits.shape\n",
    "  logits = logits.view(B*T, C)\n",
    "  targets = y.view(B*T)\n",
    "\n",
    "  # Calculate loss and update\n",
    "  loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "  loss.backward()\n",
    "\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "  optimizer.step()\n",
    "\n",
    "  loss_val = loss.item()\n",
    "  if (epoch + 1) % 500 == 0:\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_sequence, max_new_tokens, temperature=0.8):\n",
    "  model.eval()\n",
    "\n",
    "  x = start_sequence.to(model.embedding.weight.device)\n",
    "  start_text = decode(x.tolist())\n",
    "  print(start_text, end='', flush=True)\n",
    "\n",
    "  current_text = start_text\n",
    "\n",
    "  for _ in range(max_new_tokens):\n",
    "    logits, _ = model(x.view(1, -1))\n",
    "    logits = logits[0, -1, :] / temperature\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    new_char = decode([next_token.item()])\n",
    "\n",
    "    # Only add newline if it comes after a period and there's no newline already\n",
    "    if new_char == '\\n' and not current_text.endswith('.'):\n",
    "      new_char = ' '  # Replace newline with space\n",
    "\n",
    "    # Handle multiple spaces\n",
    "    if new_char == ' ' and current_text.endswith(' '):\n",
    "      continue  # Skip consecutive spaces\n",
    "\n",
    "    print(new_char, end='', flush=True)\n",
    "    current_text += new_char\n",
    "\n",
    "    x = torch.cat([x, next_token])\n",
    "\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# torch.save(model, 'nietzsche_lstm.pth')\n",
    "\n",
    "# # Load the model\n",
    "# loaded_model = torch.load('nietzsche_lstm.pth')\n",
    "# loaded_model.eval()  # Set to evaluation mode\n",
    "\n",
    "torch.save(model.state_dict(), 'nietzsche_lstm_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterLSTM(\n",
       "  (embedding): Embedding(85, 384)\n",
       "  (lstm): LSTM(384, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = CharacterLSTM(vocab_size=vocab_size, embedding_dim=384, hidden_size=512).to(device)\n",
    "saved_model.load_state_dict(torch.load('nietzsche_lstm_weights.pth'))\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus spoke Zarathustra: oper it responsibilities on.\n",
      " 1od in Loding Rolii are Were aloup out in it Betroked Wide Who Wanner anoude's Warking Wire Europeanize, Woo, in Rudance on it.\n",
      " 2eaniL Will a vowid Whatever Waking in yoowe' Will Who Begain under his virtue Were Will W lited our inward Werebling Wather and injurious Beoli gid Whole onca alone be Bodges on ea how everious either Rorigio more under Bush So Roded) We hone's Beasig Win: Who have Rudain: What is a religiou\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(encode(\"Thus spoke Zarathustra: \"), dtype=torch.long).to(device)\n",
    "generate(saved_model, context, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
