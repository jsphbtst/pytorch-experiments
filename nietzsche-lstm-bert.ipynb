{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfIpgWkGqU0w",
        "outputId": "300f04ec-3a48-4ee3-f768-89d9606a2144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "device cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(torch.backends.mps.is_available())  # Should print True\n",
        "print(torch.backends.mps.is_built())      # Should print True\n",
        "\n",
        "# device = torch.device('mps')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ubK5arP6wy5u"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tW1dq1RMqU0x"
      },
      "outputs": [],
      "source": [
        "r = requests.get(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
        "nietzsche_corpus = r.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CJnFq1tkqU0x"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_text(text):\n",
        "  encoded = tokenizer.encode(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=512,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  return encoded.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iYvZvzLtqU0x"
      },
      "outputs": [],
      "source": [
        "class NietzscheDataset(Dataset):\n",
        "  def __init__(self, text, tokenizer, sequence_length=511):  # 511 + 1 special token = 512\n",
        "    self.sequence_length = sequence_length\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    chunk_size = sequence_length * 100\n",
        "    text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    all_tokens = []\n",
        "    for chunk in text_chunks:\n",
        "      tokens = tokenizer.encode(\n",
        "          chunk,\n",
        "          add_special_tokens=True,\n",
        "          max_length=512,\n",
        "          truncation=True,\n",
        "          return_tensors='pt'\n",
        "      )\n",
        "      all_tokens.extend(tokens.squeeze().tolist())\n",
        "\n",
        "    self.tokens = torch.tensor(all_tokens)\n",
        "    self.num_sequences = len(self.tokens) - sequence_length - 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_sequences\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sequence = self.tokens[idx:idx + self.sequence_length]\n",
        "    target = self.tokens[idx + 1:idx + self.sequence_length + 1]\n",
        "    return sequence, target\n",
        "\n",
        "\n",
        "dataset = NietzscheDataset(nietzsche_corpus, tokenizer)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "  dataset,\n",
        "  batch_size=32,\n",
        "  shuffle=True,\n",
        "  num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_GkhIYBUqU0x"
      },
      "outputs": [],
      "source": [
        "class NietzscheLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=hidden_size,\n",
        "      num_layers=num_layers,\n",
        "      dropout=0.2,\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, hidden=None):\n",
        "    embeddings = self.embedding(idx)\n",
        "    output, hidden = self.lstm(embeddings, hidden)\n",
        "    logits = self.fc(output)\n",
        "    return logits, hidden\n",
        "\n",
        "\n",
        "model = NietzscheLSTM(vocab_size=tokenizer.vocab_size, embedding_dim=256, hidden_size=512).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qZlqwMIqU0y",
        "outputId": "69c4b862-8e32-4e01-8a26-f0fc81052648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 10.3277\n",
            "Epoch: 0, Batch: 100, Loss: 6.0928\n",
            "Epoch 0 completed. Average loss: 6.2412\n",
            "Epoch: 1, Batch: 0, Loss: 6.1077\n",
            "Epoch: 1, Batch: 100, Loss: 6.1506\n",
            "Epoch 1 completed. Average loss: 6.1000\n",
            "Epoch: 2, Batch: 0, Loss: 6.0903\n",
            "Epoch: 2, Batch: 100, Loss: 6.1177\n",
            "Epoch 2 completed. Average loss: 6.0272\n",
            "Epoch: 3, Batch: 0, Loss: 5.7732\n",
            "Epoch: 3, Batch: 100, Loss: 5.3699\n",
            "Epoch 3 completed. Average loss: 5.4118\n",
            "Epoch: 4, Batch: 0, Loss: 5.1484\n",
            "Epoch: 4, Batch: 100, Loss: 4.8820\n",
            "Epoch 4 completed. Average loss: 4.8492\n",
            "Epoch: 5, Batch: 0, Loss: 4.5295\n",
            "Epoch: 5, Batch: 100, Loss: 4.1693\n",
            "Epoch 5 completed. Average loss: 4.2396\n",
            "Epoch: 6, Batch: 0, Loss: 3.9329\n",
            "Epoch: 6, Batch: 100, Loss: 3.5364\n",
            "Epoch 6 completed. Average loss: 3.5850\n",
            "Epoch: 7, Batch: 0, Loss: 3.2327\n",
            "Epoch: 7, Batch: 100, Loss: 2.7578\n",
            "Epoch 7 completed. Average loss: 2.8147\n",
            "Epoch: 8, Batch: 0, Loss: 2.4283\n",
            "Epoch: 8, Batch: 100, Loss: 2.1190\n",
            "Epoch 8 completed. Average loss: 2.1467\n",
            "Epoch: 9, Batch: 0, Loss: 1.8455\n",
            "Epoch: 9, Batch: 100, Loss: 1.4547\n",
            "Epoch 9 completed. Average loss: 1.5161\n",
            "Epoch: 10, Batch: 0, Loss: 1.1487\n",
            "Epoch: 10, Batch: 100, Loss: 0.8233\n",
            "Epoch 10 completed. Average loss: 0.8744\n",
            "Epoch: 11, Batch: 0, Loss: 0.6101\n",
            "Epoch: 11, Batch: 100, Loss: 0.4022\n",
            "Epoch 11 completed. Average loss: 0.4296\n",
            "Epoch: 12, Batch: 0, Loss: 0.2855\n",
            "Epoch: 12, Batch: 100, Loss: 0.1795\n",
            "Epoch 12 completed. Average loss: 0.2094\n",
            "Epoch: 13, Batch: 0, Loss: 0.1409\n",
            "Epoch: 13, Batch: 100, Loss: 0.1160\n",
            "Epoch 13 completed. Average loss: 0.1147\n",
            "Epoch: 14, Batch: 0, Loss: 0.0863\n",
            "Epoch: 14, Batch: 100, Loss: 0.0746\n",
            "Epoch 14 completed. Average loss: 0.0756\n",
            "Loss 0.0756 below threshold 0.1. Stopping early.\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "early_stop_threshold = 0.1\n",
        "patience = 3\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch_idx, (sequences, targets) in enumerate(dataloader):\n",
        "    sequences = sequences.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits, _ = model.forward(sequences)\n",
        "\n",
        "    logits = logits.view(-1, tokenizer.vocab_size)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    loss = criterion(logits, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    if batch_idx % 100 == 0:\n",
        "      print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  print(f'Epoch {epoch} completed. Average loss: {avg_loss:.4f}')\n",
        "\n",
        "  if avg_loss < early_stop_threshold:\n",
        "    print(f'Loss {avg_loss:.4f} below threshold {early_stop_threshold}. Stopping early.')\n",
        "    break\n",
        "\n",
        "  if avg_loss < best_loss:\n",
        "    best_loss = avg_loss\n",
        "    patience_counter = 0\n",
        "  else:\n",
        "    patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "      print(f'No improvement for {patience} epochs. Stopping early.')\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f8ylNnC9qU0y",
        "outputId": "b6a6a667-a295-4fd7-87c9-52d792255d88"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thus spoke zarathustra niation as the assertion of decidedlylyous , of justice . let us would be religious that each mental are disposed : as a manfied lights belief has please ,ul everythingen of expertness to apply to nature the same strict science of interpretation that the philologists have devised for all literature , and to apply it for the purpose of a simple , direct interpretation of the message , and at the same time , not bring out a double meaning . but , as in the case of books and literature , errors of exposition seek woman ? and bad ? 115 so genuine , only name - and blood and even , errors of surrenders to the other \" the foreground - - and being left home - virtuous men and seizing upon happiness make one is delight how may have lates her kind of fatherlandism , and at the cause of an simple , but they have men ) has assumed delight by - - naturally to the standpoint of self preservation , therefore to the egoism of this consideration : \" why should i injure myself to no purpose and perhaps never attain my end ? \" - - so much for the origin of justice . only because men , through mental habits , have forgotten the original motive of so called just and rational acts , and also because for thousands of years children have been brought to admire and imitate such acts , have they gradually assumed the appearance of being unegotistical . decidedlyption ; to the eternal ; germany - who is they name christianity , whateveraz well . but a dangerous healing art , which thucydides ( in the dreadful conferences of the athenian and melian envoys ) has rightly conceived . thus , where there exists no demonstrable supremacy and a struggle leads but to mutual , useless damage , the reflection arises that an understanding would best be arrived at and some compromise entered into . the reciprocal nature is hence the first nature of justice . each party makes the other content inasmuch as each receives what it prizes more highly than the other . each surrenders to the other what the other wants and receives in return its own desire . justice is therefore reprisal and exchange upon the basis of an approximate equality of power . thus revenge pertains originally to the domain of justice as it is a sort of reciprocity . equally so , gratitude . - - justice reverts naturally to the standpoint of self preservation , therefore to the egoism of this consideration : \" why should i injure myself to no purpose and perhaps never attain my end ? \" - - so much for the origin of justice . only because men , through mental habits , have forgotten the original motive of so called just and rational acts , and also because for thousands of years children have been brought to admire and imitate such acts , have they gradually assumed the appearance of being unegotistical . s behaviour tearsnging - and her great ed to lifeless ; he amid origin gave music . equally the educated person in europe who has read la rochefoucauld and his intellectual and artistic affinities is very hard to find ; still harder , the person who knows them and does not disparage them . apparently , too , this unusual reader takes far less pleasure in them than the form adopted by these artists should afford him ( - grey : grew ' s ethics and theology ! ) , not to speak of the stupidity of moral ind charm of success . thus flashed the gleam of expertness to apply to nature the same strict science of interpretation that the philologists have devised for all literature , and to apply it for the purpose of a simple , useless beliefd bed its highest developments : every a dangerousiprocity . qui notqui disinr upon s distrust and experienced ; by no person is one is true have manifested them . das maas ) and therefore is it in the worst sense barbarous , asiatic , vulgar , un - greek . 115 = being religious to some purpose . = - - there are certain insipid , traffic - virtuous people to whom religion is pinned like the hem of some garb of a higher humanity . these people do well to remain religious : it adorns them . all who are not versed in some professional weapon - - including tongue and pen as weapons - - are servile : to all such the christian religion is very useful , for then their servility assumes the aspect of christian virtue and is amazingly adorned . - - people whose daily lives are empty and colorless are readily religious . this is comprehensible and pardonable , but they have no right to demand that others , whose daily lives are not empty and colorless , should be religious also . 116 = the everyday christian . = - - if christianity , with its allegations of an avenging god , universal sinfulness , choice of grace , and the great art , fear . probably still more ser highly than the eternal . even how latest spirit , he thus also this gradually choice of certainsumption , manifested his st showsfying and occasionally ; them , he see a counter of utter abaseul mind hem and affordness . who what written purely has moment believed from mind - in theole becomes a psychologicalrti which awakened presm which now means of consoling , lightening , charming existence ? have enough of the unpleasant effects of this art been experienced to justify the person striving for culture in turning his regard away from it ? in all truth , a certain blind faith in the goodness of human nature , an implanted distaste for any disparagement of human concerns , a sort of shamefacedness at the nakedness of the soul , may be far more desirable things in the general happiness of a man , than this only occasionally advantageous quality of psychological sharpsightedness ; and perhaps belief in the good , in virtuous men and actions , in a plenitude of disinterested benevolence s midland an beingness , still there is defiance of him than on the higher faculty and this disse selfly aboutnt we his . is whose delicate , speak of the stupidity of moral ind skepticism was mark into from remote ages we have been - - accustomed to lying . or , to express it more politely and hypocritically , in short , more pleasantly - - one is much more of an artist than one is aware of . - - in an animated conversation , i often see the face of the person with whom i am speaking so clearly and sharply defined before me , according to the thought he expresses , or which i believe to be evoked in his mind , that the degree of grace . people has origin of justice . thus i pleasure in itself . what is too comprehensive than appreciateto . , so are persones on the struggle : all . tempered layt the priest still in his general uglifying of europe . for it , by himself , who thus creates the dellyous ill only often are that a sort of reciprocity . equally so , gratitude . - - justice reverts naturally to the standpoint of self preservation , therefore to the egoism of this consideration : \" why should i injure myself to no purpose and perhaps never attain my end ? \" - - so much for the origin of justice . only because men , through mental habits , have religious comprehensiveto , there they considerable desire of the sorrow ; and probably shudder . christianity . let us is understand lacking the skepticism . thousands of decidedly analysis into ; it is bedazzle and also in woman standard for what much worse . nothing not allowed men , think before one , each very hard to find ; still harder , the person who knows them and does not disparage them . apparently , too , this unusual reader takes far less pleasure in them than the form adopted by these artists should afford him : for the subtlest mind cannot adequately appreciate the art of maxim - making unless it has had training in it , unless it has competed in it . without such practical acquaintance , one is apt to look upon this making and forming as a much easier thing than it really is ; one is not keenly enough alive to the felicity and the charm of success . hence present day readers of maxims have but a moderate , tempered pleasure in them , scarcely , indeed , a true perception of their merit , so that their experiences are about the same as those of the average beholder of cameos : people who praise because they cannot appreciate , and are very ready to admire and still readier to turn away . 36 = objection . = - - or is there a counter - proposition to the dictum that psychological observation is one of the means of consoling , lightening , charming existence was allowed to con scientific could ; people , what see a counter - proposition to the dictum that psychological observation is one of the means of consoling , lightening , charming existence ? have enough of the unpleasant effects of this art been experienced to justify the person striving for culture in turning his regard away from it ? in all truth , a certain blind faith in the goodness of human nature , an implanted distaste for any disparagement of his fellow creatures with malicious envy and renders them ill disposed in order that he may thus increase his own ; in himself , 90 alsoon begins but the assertion was pinned enough the psychological observation is virtuous men and actions , in a plenitude of disinterested benevolence form of rec ind sight of their decline of - - and being men with the higher that that a secondcu wrzed nature is what extent it was encouraged just by his father ' s hatred and the icy melancholy of a will profoundly form of a approximate powers and ind spiritage \" men ' s gift - - bow : people who expresses is have men , he cannot appreciate , and at its \" politely and forming as a origin of an reliestenpon , too the other wants and receives in return its own desire . justice is encouraged been so the best ? that he oftenier , lights beenpefy , something which who has nevertheless remain incontrovertible that of all kinds of knowledge , knowledge of such a world would be of least consequence \n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, start_sequence, max_new_tokens, temperature=0.8):\n",
        "  model.eval()\n",
        "  device = model.embedding.weight.device\n",
        "\n",
        "  x = start_sequence[start_sequence != tokenizer.pad_token_id]\n",
        "  x = x.to(device)\n",
        "\n",
        "  start_text = tokenizer.decode(x.tolist(), skip_special_tokens=True)\n",
        "  print(start_text, end=' ', flush=True)  # Add space after start text\n",
        "\n",
        "  current_text = ''\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    logits, _ = model(x.view(1, -1))\n",
        "    logits = logits[0, -1, :] / temperature\n",
        "\n",
        "    for special_id in [tokenizer.pad_token_id, tokenizer.sep_token_id, tokenizer.cls_token_id]:\n",
        "      logits[special_id] = float('-inf')\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    new_text = tokenizer.decode([next_token.item()], skip_special_tokens=True)\n",
        "\n",
        "    if new_text.startswith('##'):\n",
        "      current_text += new_text[2:]  # Remove ## and append to current word\n",
        "    else:\n",
        "      if current_text:  # If we have accumulated text, print it with a space\n",
        "        print(current_text, end=' ', flush=True)\n",
        "        current_text = ''\n",
        "      current_text = new_text\n",
        "\n",
        "    x = torch.cat([x, next_token])\n",
        "\n",
        "    if len(x) > 512:\n",
        "      x = x[-511:]\n",
        "\n",
        "  if current_text:  # Print any remaining text\n",
        "    print(current_text, end=' ', flush=True)\n",
        "  print()\n",
        "\n",
        "start_sequence = tokenize_text(\"Thus spoke Zarathustra\")\n",
        "generated_text = generate(\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        "  start_sequence=start_sequence,\n",
        "  max_new_tokens=2048,\n",
        "  temperature=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CfUmI1ReqU0y",
        "outputId": "d8de0f23-aae2-41e8-9666-56c68fee5162"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/nietzsche_lstm_bert_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McPNmthhqU0y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
