{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfIpgWkGqU0w","executionInfo":{"status":"ok","timestamp":1739951494244,"user_tz":-60,"elapsed":4716,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"outputId":"c1702690-89f9-439b-9d7c-f881138fd4b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","False\n","device cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import requests\n","from transformers import AutoTokenizer\n","\n","print(torch.backends.mps.is_available())  # Should print True\n","print(torch.backends.mps.is_built())      # Should print True\n","\n","# device = torch.device('mps')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('device', device)"]},{"cell_type":"code","source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"ubK5arP6wy5u","executionInfo":{"status":"ok","timestamp":1739951496378,"user_tz":-60,"elapsed":3,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tW1dq1RMqU0x","executionInfo":{"status":"ok","timestamp":1739951499541,"user_tz":-60,"elapsed":1701,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"45d9813d-6c9f-4b9a-cb80-b271baeca1cb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["r = requests.get(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n","nietzsche_corpus = r.text"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CJnFq1tkqU0x","executionInfo":{"status":"ok","timestamp":1739951502017,"user_tz":-60,"elapsed":984,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"844e1adc-4bb8-4f60-f45b-c97cf0fe9775"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","def tokenize_text(text):\n","  encoded = tokenizer.encode(\n","      text,\n","      add_special_tokens=True,\n","      max_length=512,\n","      padding='max_length',\n","      truncation=True,\n","      return_tensors='pt'\n","  )\n","\n","  return encoded.squeeze()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iYvZvzLtqU0x","executionInfo":{"status":"ok","timestamp":1739951506626,"user_tz":-60,"elapsed":519,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"da06b51e-e66c-49d8-da40-8d14be2a4ffc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["class NietzscheDataset(Dataset):\n","  def __init__(self, text, tokenizer, sequence_length=511):  # 511 + 1 special token = 512\n","    self.sequence_length = sequence_length\n","    self.tokenizer = tokenizer\n","\n","    chunk_size = sequence_length * 100\n","    text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n","\n","    all_tokens = []\n","    for chunk in text_chunks:\n","      tokens = tokenizer.encode(\n","          chunk,\n","          add_special_tokens=True,\n","          max_length=512,\n","          truncation=True,\n","          return_tensors='pt'\n","      )\n","      all_tokens.extend(tokens.squeeze().tolist())\n","\n","    self.tokens = torch.tensor(all_tokens)\n","    self.num_sequences = len(self.tokens) - sequence_length - 1\n","\n","  def __len__(self):\n","    return self.num_sequences\n","\n","  def __getitem__(self, idx):\n","    sequence = self.tokens[idx:idx + self.sequence_length]\n","    target = self.tokens[idx + 1:idx + self.sequence_length + 1]\n","    return sequence, target\n","\n","\n","dataset = NietzscheDataset(nietzsche_corpus, tokenizer)\n","\n","dataloader = DataLoader(\n","  dataset,\n","  batch_size=32,\n","  shuffle=True,\n","  num_workers=0\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_GkhIYBUqU0x","executionInfo":{"status":"ok","timestamp":1739951509287,"user_tz":-60,"elapsed":527,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"fca32284-449a-4057-8d41-ce431b58350d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}],"source":["class NietzscheLSTM(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=3, num_heads=8, dropout=0.1):\n","    super().__init__()\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.lstm = nn.LSTM(\n","      input_size=embedding_dim,\n","      hidden_size=hidden_size,\n","      num_layers=num_layers,\n","      dropout=0.2,\n","      batch_first=True\n","    )\n","    self.attention = nn.MultiheadAttention(\n","      embed_dim=hidden_size,\n","      num_heads=num_heads,\n","      dropout=dropout,\n","      batch_first=True\n","    )\n","\n","    self.layer_norm = nn.LayerNorm(hidden_size)\n","\n","    self.dropout = nn.Dropout(dropout)\n","\n","    self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","  def forward(self, idx, hidden=None):\n","    embeddings = self.embedding(idx)\n","    output, hidden = self.lstm(embeddings, hidden)\n","    attn_output, attn_weights = self.attention(\n","      query=output,\n","      key=output,\n","      value=output\n","    )\n","    attn_output = self.layer_norm(output + attn_output)\n","    attn_output = self.dropout(attn_output)\n","    logits = self.fc(attn_output)\n","    return logits, hidden, attn_weights\n","\n","\n","model = NietzscheLSTM(vocab_size=tokenizer.vocab_size, embedding_dim=256, hidden_size=512).to(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"2qZlqwMIqU0y","executionInfo":{"status":"ok","timestamp":1739951677206,"user_tz":-60,"elapsed":164077,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"outputId":"dbe735a5-4b8b-480f-ee94-9a6cd854e52e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 0, Batch: 0, Loss: 10.4688\n","Epoch: 0, Batch: 100, Loss: 0.1071\n","Epoch 0 completed. Average loss: 1.3464\n","Epoch: 1, Batch: 0, Loss: 0.0409\n","Epoch: 1, Batch: 100, Loss: 0.0256\n","Epoch 1 completed. Average loss: 0.0275\n","Epoch: 2, Batch: 0, Loss: 0.0179\n","Epoch: 2, Batch: 100, Loss: 0.0171\n","Epoch 2 completed. Average loss: 0.0163\n","Epoch: 3, Batch: 0, Loss: 0.0125\n","Epoch: 3, Batch: 100, Loss: 0.0144\n","Epoch 3 completed. Average loss: 0.0122\n","Epoch: 4, Batch: 0, Loss: 0.0087\n","Epoch: 4, Batch: 100, Loss: 0.0104\n","Epoch 4 completed. Average loss: 0.0100\n","Epoch: 5, Batch: 0, Loss: 0.0085\n","Epoch: 5, Batch: 100, Loss: 0.0092\n","Epoch 5 completed. Average loss: 0.0088\n","Loss 0.0088 below threshold 0.01. Stopping early.\n"]}],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n","\n","num_epochs = 50\n","early_stop_threshold = 0.01\n","patience = 3\n","best_loss = float('inf')\n","patience_counter = 0\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","  total_loss = 0\n","\n","  for batch_idx, (sequences, targets) in enumerate(dataloader):\n","    sequences = sequences.to(device)\n","    targets = targets.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    logits, _, _ = model.forward(sequences)\n","\n","    logits = logits.view(-1, tokenizer.vocab_size)\n","    targets = targets.view(-1)\n","\n","    loss = criterion(logits, targets)\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    total_loss += loss.item()\n","    if batch_idx % 100 == 0:\n","      print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n","\n","  avg_loss = total_loss / len(dataloader)\n","  print(f'Epoch {epoch} completed. Average loss: {avg_loss:.4f}')\n","\n","  if avg_loss < early_stop_threshold:\n","    print(f'Loss {avg_loss:.4f} below threshold {early_stop_threshold}. Stopping early.')\n","    break\n","\n","  if avg_loss < best_loss:\n","    best_loss = avg_loss\n","    patience_counter = 0\n","  else:\n","    patience_counter += 1\n","    if patience_counter >= patience:\n","      print(f'No improvement for {patience} epochs. Stopping early.')\n","      break"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"f8ylNnC9qU0y","executionInfo":{"status":"ok","timestamp":1739951767885,"user_tz":-60,"elapsed":37909,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"outputId":"6944d196-bd42-4a6e-ed21-d21c7a082b2b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["thus spoke zarathustra its little do - - a super - european music , which holds its own even in presence of the brown sunsets of the desert , whose soul is akin to the palm - tree , and can be at home and can roam with big , beautiful , lonely beasts of prey . . . i could imagine a music of which the rarest charm would be that it knew nothing more of good and evil ; only that here and there perhaps some sailor ' s home - sickness , some analysis \" ( ' s taking the expression in its widest sense ) perhaps not be the exception , but the rule ? - - perhaps genius is by no means so rare : but rather the five hundred hands which it requires in order to tyrannize over the [ greek inserted here ] , \" the right time \" - - in order to take chance by the forelock ! 275 . he who does not wish to see the height of a man , looks all the more sharply at what is low in him , and in the foreground - - and thereby betrays himself . 276 . in all kinds of injury and loss the lower and coarser soul is better off than the nobler soul : the dangers of the latter must be greater , the probability that it will come to grief and perish is in fact immense , considering the multiplicity of the conditions of its existence . - - in a lizard a finger grows again which has been lost ; not so in man . - - 277 . it is too bad ! always the old story ! when a man has finished building his house , he finds that he has learnt unawares something which he ought absolutely to have known before he - - began to build . the eternal , fatal \" too late ! \" the melancholia of everything completed - - ! 278 . - - wanderer , who art thou ? i see thee follow thy path without scorn , without love , with unfathomable eyes , wet and sad as a plummet which has returned to the light insatiated out of every depth - - what did it seek down there ? - - with a bosom that never sighs , with lips that conceal their loathing , with a hand which only slowly grasps : who art thou ? what hast thou done ? rest thee here : this place has hospitality for every one - - refresh thyself ! and whoever thou art , what is it that now pleases thee ? what will serve to refresh thee ? only name it , whatever i have i offer thee ! \" to refresh me ? to refresh me ? oh , thou prying one , what sayest thou ! but give me , i pray thee - - - \" what ? what ? speak out ! \" another mask ! a second mask ! \" 279 . men of profound sadness betray themselves when they are happy : they have a mode of seizing upon happiness as though they would choke and strangle it , out of jealousy - - ah , they know only too well that it will flee flee must sed , if all - day . 8 = pneumatic explanation of nature . = [ 5 ] - - metaphysic reads the message of nature as if it were written purely pneumatically , as the church and its learned ones formerly did where the bible was concerned . it requires a great deal of expertness to apply to nature the same strict science of interpretation that the philologists have devised for all literature , and to apply it for the purpose of a simple , direct interpretation of the message , and at the same time , not bring out a double meaning . but , as in the case of books and literature , errors of exposition are far from being completely eliminated , and vestiges of allegorical and mystical interpretations are still to be met with in the most cultivated circles , so where nature is concerned the case is - - actually much worse . [ 5 ] pneumatic is here used in the sense of spiritual . pneuma being the greek word in the new testament for the holy spirit . - - ed . 9 = metaphysical world . = - - it is true , there may be a metaphysical world ; the absolute possibility of it can scarcely be disputed . we see all things through the medium of the human head and we cannot well cut off this head : although there remains the question what part of the world would be left after it had been cut off . but that is a purely abstract scientific problem and one not much calculated to give men uneasiness : yet everything that has heretofore made metaphysical assumptions valuable , fearful or delightful to men , all that gave rise to them is passion , error and self deception : the worst systems of knowledge , not the best , pin their tenets of belief thereto . when such methods are once brought to view as the basis of all existing religions and metaphysics , they are already discredited . there always remains , however , the possibility already conceded : but nothing at all can be made out of that , to say not a word about letting happiness , salvation and life hang upon the threads spun from such a possibility . accordingly , nothing could be predicated of the metaphysical world beyond the fact that it is an elsewhere , [ 6 ] another sphere , inaccessible and incomprehensible to us : it would become a thing of negative properties . even were the existence of such a world absolutely established , it would nevertheless remain incontrovertible that of all kinds of knowledge , knowledge of such a world would be of least consequence - - of even less consequence than knowledge of the chemical analysis analysis thee s home - sickness , some interpretation the philologists have devised for all literature , and to apply it for the purpose of a simple , direct interpretation of the message , and at the same time , not bring out a double meaning . but , as in the case of books and literature , errors of exposition are far from being completely eliminated , and vestiges of allegorical and mystical interpretations are still to be met with in the most cultivated circles , so where nature is concerned the case is - - actually much worse . [ 5 ] pneumatic is here used in the sense of spiritual . pneuma being the greek word in the new testament for the holy spirit . - - ed . 9 = metaphysical world . = - - it is true , there may be a metaphysical world ; the absolute possibility of it can scarcely be disputed . we see all things through the medium of the human head and we cannot well cut off this head : although there remains the question what part of the world would be left after it had been cut off . but that is a purely abstract scientific problem and one not much calculated to give men uneasiness : yet everything that has heretofore made metaphysical assumptions valuable , fearful or delightful to men , all that gave rise to them is passion , error and self deception : the worst systems of knowledge , not the best , pin their tenets of belief thereto . when such methods are once brought to view as the basis of all existing religions and metaphysics , they are already discredited . there always remains , however , the possibility already conceded : but nothing at all can be made out of that , to say not a word about letting happiness , salvation and life hang upon the threads spun from such a possibility . accordingly , nothing could be predicated of the metaphysical world beyond the fact that it is an elsewhere , [ 6 ] another sphere , inaccessible and incomprehensible to us : it would become a thing of negative properties . even were the existence of such a world absolutely established , it would nevertheless remain incontrovertible that of all kinds of knowledge , knowledge of such a world would be of least consequence - - of even less consequence than knowledge of the chemical analysis analysis s psychological analysis ' s behaviour towards children ! - - which has really been best restrained and dominated hitherto by the fear of man . alas , if ever the \" eternally tedious in woman \" - - she has plenty of it ! - - is allowed to venture forth ! if she begins radically and on principle to unlearn her wisdom and art - of charming , of playing , of frightening away sorrow , of alleviating and taking easily ; if she forgets her delicate aptitude for agreeable desires ! female voices are already raised , which , by saint aristophanes ! make one afraid : - - with medical explicitness it is stated in a threatening manner what woman first and last requires from man . is it not in the very worst taste that woman thus sets herself up to be scientific ? enlightenment hitherto has fortunately been men ' s affair , men ' s gift - - we remained therewith \" among ourselves \" ; and in the end , in view of all that women write about \" woman , \" we may well have considerable doubt as to whether woman really desires enlightenment about herself - - and can desire it . if woman does not thereby seek a new ornament for herself - - i believe ornamentation belongs to the eternally feminine ? - - why , then , she wishes to make herself feared : perhaps she thereby wishes to get the mastery . but she does not want truth - - what does woman care for truth ? from the very first , nothing is more foreign , more repugnant , or more hostile to woman than truth - - her great art is falsehood , her chief concern is appearance and beauty . let us confess it , we men : we honour and love this very art and this very instinct in woman : we who have the hard task , and for our recreation gladly seek the company of beings under whose hands , glances , and delicate follies , our seriousness , our gravity , and profundity appear almost like follies to us . finally , i ask the only rovencal and ligurian blood froths over , preserves them from the dreadful , northern grey - in - grey , from sunless conceptual - spectrism and from poverty of blood - - our german infirmity of taste , for the excessive prevalence of which at the present moment , blood and iron , that is to say \" high politics , \" has with \n"]}],"source":["@torch.no_grad()\n","def generate(model, tokenizer, start_sequence, max_new_tokens, temperature=0.8):\n","  model.eval()\n","  device = model.embedding.weight.device\n","\n","  x = start_sequence[start_sequence != tokenizer.pad_token_id]\n","  x = x.to(device)\n","\n","  start_text = tokenizer.decode(x.tolist(), skip_special_tokens=True)\n","  print(start_text, end=' ', flush=True)  # Add space after start text\n","\n","  current_text = ''\n","\n","  for _ in range(max_new_tokens):\n","    logits, _, _ = model.forward(x.view(1, -1))\n","    logits = logits[0, -1, :] / temperature\n","\n","    for special_id in [tokenizer.pad_token_id, tokenizer.sep_token_id, tokenizer.cls_token_id]:\n","      logits[special_id] = float('-inf')\n","\n","    probs = torch.softmax(logits, dim=-1)\n","    next_token = torch.multinomial(probs, num_samples=1)\n","\n","    new_text = tokenizer.decode([next_token.item()], skip_special_tokens=True)\n","\n","    if new_text.startswith('##'):\n","      current_text += new_text[2:]  # Remove ## and append to current word\n","    else:\n","      if current_text:  # If we have accumulated text, print it with a space\n","        print(current_text, end=' ', flush=True)\n","        current_text = ''\n","      current_text = new_text\n","\n","    x = torch.cat([x, next_token])\n","\n","    if len(x) > 512:\n","      x = x[-511:]\n","\n","  if current_text:  # Print any remaining text\n","    print(current_text, end=' ', flush=True)\n","  print()\n","\n","start_sequence = tokenize_text(\"Thus spoke Zarathustra\")\n","generated_text = generate(\n","  model=model,\n","  tokenizer=tokenizer,\n","  start_sequence=start_sequence,\n","  max_new_tokens=2048,\n","  temperature=0.8\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"CfUmI1ReqU0y","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1739951818278,"user_tz":-60,"elapsed":22366,"user":{"displayName":"Joseph J. Bautista","userId":"17191999425584596332"}},"outputId":"686a51d3-ce36-46ee-e1f1-cbaedc74b330"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","torch.save(model.state_dict(), '/content/drive/My Drive/nietzsche_lstm_bert_attention_weights.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McPNmthhqU0y"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.1"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}