{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u27xeVWzbnoW",
        "outputId": "bf55e45c-9d1b-49a1-9dc9-c0caad33fdb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())  # Check if CUDA (GPU) is available\n",
        "print(torch.cuda.device_count()) # Print number of available GPUs\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import requests\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAa5hiDYbnoX",
        "outputId": "d3bc0ffa-4311-44a3-b775-e963d0f6be78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x783fc440e730>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 50\n",
        "learning_rate = 1e-2\n",
        "eval_iters = 200\n",
        "\n",
        "# GPT-1 hyperparameters\n",
        "n_embd = 768  # embedding dimension (was smaller in Karpathy's version)\n",
        "n_head = 12   # number of attention heads (was fewer)\n",
        "n_layer = 12  # number of transformer blocks (was fewer)\n",
        "block_size = 512  # context window (matches GPT-1)\n",
        "dropout = 0.1    # dropout rate\n",
        "\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RqeCGjW9bnoX"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "r = requests.get(url=url)\n",
        "text = r.text\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gfdzhO9obnoX"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)    # (B,T,C)\n",
        "    q = self.query(x)  # (B,T,C)\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x)  # (B,T,C)\n",
        "    out = wei @ v  # (B, T, C)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # Add layer normalization\n",
        "    self.norm = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" GPT-1 style feed-forward network \"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(n_embd, 4 * n_embd),\n",
        "      nn.GELU(),  # GPT-1 used GELU instead of ReLU\n",
        "      nn.Linear(4 * n_embd, n_embd),\n",
        "      nn.Dropout(dropout),\n",
        "    )\n",
        "    # Add layer normalization\n",
        "    self.norm = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    # GPT-1 used layer norm before attention and ffwd (pre-norm)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Change order of layer norm (pre-norm)\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class GPT1Model(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # Token embeddings\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "    # Transformer blocks\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "    # Final layer norm\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "\n",
        "    # Language model head\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    # Better initialization (as used in GPT-1)\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "      torch.nn.init.zeros_(module.bias)\n",
        "      torch.nn.init.ones_(module.weight)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # Get embeddings\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    # Apply transformer blocks\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "\n",
        "    # Get logits\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    # Calculate loss if targets provided\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, _ = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] / temperature\n",
        "\n",
        "      # optionally crop probabilities to only the top k options\n",
        "      if top_k is not None:\n",
        "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "      # apply softmax to convert logits to probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      # print the generated token\n",
        "      print(decode([idx_next[0].item()]), end='', flush=True)\n",
        "\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    print()  # new line at the end\n",
        "    return idx\n",
        "\n",
        "model = GPT1Model(vocab_size=vocab_size)\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N4_qdUOPbnoY"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters).to(device)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D15VcGr7bnoY",
        "outputId": "55333c6f-19fc-4986-8ae8-00b027e6f96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uIe\n",
            "sd A \n",
            "  u sea\n",
            "rO;ahes ad \n",
            "akeosoe \n",
            "U \n",
            "T    Go eg,oa eh  y fk\n",
            ",\n",
            "  ui t\n",
            "tdoef\n",
            "l  yntfioh\n",
            " e twt -\n",
            " dgie wnpeetn  me\n",
            "aleAs.? oh r h e s rse eU   tB\n",
            "BeA \n",
            "Y gaTohnelhw\n",
            "oetee ri:oekaelcteeeefe U ose e n\n",
            "step 0: train loss 3.3527, val loss 3.3916\n",
            "ue oPsroiea anuPgp u Y;clfHndsBtulUnidoona has, oBr nmnh. b: eyIB d  oitm o'ih'rU;kmano o:nilwroo esnyi dnn.rs iog\n",
            "u t paeeratOntlt.eYu tes\n",
            "f eudv lnsoGeseen foa Inhud tprv weecReh noaam O sn\n",
            "\n",
            "e  tn i\n",
            "step 50: train loss 3.3205, val loss 3.3597\n",
            "ael go V t ghhd-\n",
            "CR r nsD\n",
            "YySee dr,\n",
            ", t seeiimiflisisenkig aor l dond cmsomN;Pe,  te bai'\n",
            "\n",
            "T\n",
            "L efolhi Do?:T'd heviohteab ?NSebn d nle k t yF fd uaths s\n",
            "I socheehts d cr,sn .ut,EOtls slLona;atoperas ws\n",
            "step 100: train loss 3.0422, val loss 3.0539\n",
            "oian d  I ,he hsnd:\n",
            "\n",
            " wean vamis sek.War\n",
            "hyh m canlhneene w onlpedeowmrutejl tasoanle t nshend fdees oyh t r teru ddRA ly:hoaaa' egteamshe inamyeang reenrtcoctsounettisyir thad\n",
            " othau pe d ble rn, yhd\n",
            "step 150: train loss 2.8802, val loss 2.8920\n",
            "ae oteshrllr alt oct hherleroerrotsm farerebatuer hor t veky fa  he,odandE: cekns'at! ty a cinilhe lun s we hato I:\n",
            "Mitoaf digt areey t f:\n",
            "T\n",
            "T iaandholowep nd nawlekle wol ane hleasnleol.\n",
            "lus aviatoio\n",
            "step 200: train loss 2.7500, val loss 2.7582\n",
            "Ele sesclar, I\n",
            "Denong le howdteglrstoner iqath aevame? ig:'t, nepsl si m VTnvoudled ico 'ry m, th feor s amoassith sraatr at imandlk oo; hip\n",
            "Th, t g bisitythirimy h y lal ouete?\n",
            "MER, mstaga se\n",
            "Cha:\n",
            "P:\n",
            "step 250: train loss 2.6926, val loss 2.6858\n",
            "ARsomean:\n",
            "\n",
            "of seyon\n",
            "Is w, arn I w y\n",
            "TA yorees POothmw mo. d nd he y rvoicred yon l toV outot I eouresinshekes.\n",
            "Ton hin larrt Ious:\n",
            "AONS: lo nouud wttkaved athis?\n",
            "RW be?\n",
            "NCEENac!\n",
            "G nthevowandpeweyer:, \n",
            "step 300: train loss 2.6323, val loss 2.6288\n",
            "OI athstloe nis:N\n",
            "H:\n",
            "\n",
            "SBou-h bey.\n",
            "\n",
            "\n",
            "GEx:\n",
            "\n",
            "n, seer mgorof s pedcoo thyoutsladelrstn henn pad hea tha heeas d.\n",
            "Yupouthas at Gvalediofasteloqe 'e t bithinUAabe ath hy t lells, b fsit:\n",
            "LONI m\n",
            "ETkitey s o \n",
            "step 350: train loss 2.6217, val loss 2.6243\n",
            " hy watheiYea,\n",
            "IO thala an and coourtheay waghok' s,\n",
            "Ano.BESallepeslpu cy ath RI b h wounonc wo tisor mrel,\n",
            "YIoret\n",
            "Clis bon, sangwetozis an. im,\n",
            "PUIanere nd?\n",
            "\n",
            "I giJilous som nonouant h rtoe thoouteer,\n",
            "step 400: train loss 2.6007, val loss 2.5974\n",
            "o Coimb't, ontis yo s mansutheF V, 'ng mhe pgt th thnicy'd gam b'khhe,\n",
            "cqm nme\n",
            "Po ar tino wiithaopr.\n",
            "\n",
            "\n",
            "Uoft. tifo Wr tot at treur,\n",
            " Pu who eenou sthe be aw me c, whisisilaced?\n",
            " sovay dn, itre\n",
            "Mmpieabu\n",
            "step 450: train loss 2.5997, val loss 2.6002\n",
            "EA:ore hy Mad bos,\n",
            "Tea ce minekh leyot;\n",
            "\n",
            "HOeacleld\n",
            "Tound iord kOmave mressup monap:\n",
            "HolHDUTh an y an'tin.\n",
            "\n",
            "GA mbfomo onouy dieangelll ENS:\n",
            "\n",
            "Wanve th tingrathownderdritngOoren gung g dleane tlay thidst\n",
            "step 500: train loss 2.5896, val loss 2.5898\n",
            " roRre owomivoironther th corest qtle:othe ur ou merourdhneoaris.\n",
            "'ryo oty rt md that eotofac ty\n",
            "LI lisees\n",
            "O:f ero I w\n",
            "Mevoufont Isererarid cheimon ltI to y youmere pthuy,\n",
            "The a le, w ar ande\n",
            "A, INI d\n",
            "step 550: train loss 2.6182, val loss 2.6221\n",
            "DTheve na mor\n",
            "I t he s gdtis d UI's ys by wbou t, ud n thenis ha sigeiourereidre n, o t!k Vtitrdee 'ametod mefered; thithan g',s toseco n: Yighinsulomayesnsitinou,! toce thaithesouven y h Isayo bowe h\n",
            "step 600: train loss 2.5820, val loss 2.5832\n",
            "ukichern t th ef he ser j ts\n",
            "be f s CE Igaiciithis\n",
            "STUthoumu the, y ar stind 'me fareiISo anvet mered tiVe.\n",
            "\n",
            "CEet harses f iior t br d walooloupay!Ws h  g, taANGLUELIOMo thithiknconiu h bteous me e ho\n",
            "step 650: train loss 2.5743, val loss 2.5704\n",
            "T \n",
            "Oprel ainoIVIELAn lt nk lel cavetom CHE.\n",
            "\n",
            "A ld offirod s me rag neardl g beem t BakRL he amarid:\n",
            "\n",
            "patis fRoaussegis,\n",
            "\n",
            "INEs alacuangm\n",
            "WAtheshe nd and willoary?\n",
            "Dr,\n",
            "Thmberlld th murst! g mj th If ofd\n",
            "step 700: train loss 2.5818, val loss 2.5900\n",
            "u s l panNofabllofloove r helle:\n",
            "HTly be s. m flesAN werend,eth war; weal hmell ghs:\n",
            "Pr, waliusean\n",
            "Ar ffofave cpuwithibes mEuseanderatou bofis fenunstcanualy'n poth yo arsesif hethopo t marde e lt con\n",
            "step 750: train loss 2.5761, val loss 2.5761\n",
            "ICerdbie t tiot ye' aththovis?\n",
            "F\n",
            "Bnd gon:\n",
            "ched Yilee s thowo them no m sh\n",
            "Ab n:\n",
            "Ie, herter wnheout n\n",
            "Kawe hounthoin w B:\n",
            "Pnsave ot:\n",
            "Opy r'sstor.ou atinroaifis athe\n",
            "TUSoshinket's l g,\n",
            "ET' y d'e rsoman:\n",
            "step 800: train loss 2.5922, val loss 2.5905\n",
            "ICM: thuids w seut d wis nd,\n",
            "Os 's tUI'r p aeg, WiLod ime thero, b urerThar fy; shropuh sor me\n",
            "Mefto fey lor wougserk hetondy g s wathin uroo worere s;\n",
            "Tend the the t stins y t,\n",
            "bec d l b nathon ainol\n",
            "step 850: train loss 2.5817, val loss 2.5848\n",
            "DenomiI w. ee domy lthamd s a tehlnoorowi: terowfu d doue ws\n",
            "F y?\n",
            "E: terShig? yoo?\n",
            "alaAUang Ie che a oaeetileistulie.\n",
            "ManN\n",
            "GW bsou-STorofoseat  lt thio hanoouan, s mofee.\n",
            "\n",
            "Thavh; d atume ae dink.\n",
            "Iste\n",
            "step 900: train loss 2.6297, val loss 2.6324\n",
            " w\n",
            "T\n",
            "irhanyi,\n",
            "ar IDI reow I Ot eureopcer ithostyite lwirhhak,\n",
            "Wo bl! om ure poos thid eI gsorthortvu buth ansat:os thicorest oondou he flobes, ne nd allona t,\n",
            "E,\n",
            "P.\n",
            "\n",
            "ALhouuple mmeoudird t ure foy tepe\n",
            "step 950: train loss 2.6446, val loss 2.6499\n",
            "hh\n",
            "ed h wom,g\n",
            "WIilhhtousar t nisrshore, ne kr MRODree weor mamurgdallor disey\n",
            "Y rnc hoes wr mby:\n",
            "CEB\n",
            "HL, ode:\n",
            "Shth\n",
            "Mha this d LThit\n",
            "R.\n",
            "\n",
            "M\n",
            "Hyo tam,te mn yor asone\n",
            "Sth inor.\n",
            "On llt ac worrrewile tehed b\n",
            "step 999: train loss 2.6564, val loss 2.6677\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    model.generate(context, max_new_tokens=200)\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UINhIDJrbnoY",
        "outputId": "9df5d40e-c572-4033-cc8b-d1f48e32fb6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " opnd yelal trs\n",
            "e s? w tom t\n",
            "L\n",
            "Ccuid h I d ro tianeepot hhasoueror d ceptitor nerovet ld mceenre s, ts arororus atofenouer mesttom idng sse bhom mis ln a tenoufo'nte o s\n",
            "Mnn is n  ainand menekenais go ke whapar a f rest s'sh uendn ansth bea dercaey Ieanlacovotit lareon wigut w' eutathpinast thin\n",
            "B\n",
            "Odendstiai'ouiveanot; dyoubes oowe!B\n",
            "Dtor bee he?\n",
            "Whr iccengofortuny d otor w scrayosh sive thamW orinc., ith binor, msig h y en l ccowhindeeabhoritilise owo t.\n",
            "or arr ncie ur,\n",
            "Ont sand g me doulie t:\n",
            "\n",
            "Sy s\n",
            "CBas,etatefot, lngithe t be WEhe icrineloowterat th,AWhane s mmnythinde,olishemeRGhat  s thth,n ebrdelloo the t her;puy\n",
            "\n",
            "sewcheugcWglellld ow:\n",
            "IUF hee\n",
            "Ifldre t perth we,in, s s br wotcele \n",
            "\n",
            "J US:\n",
            "\n",
            "\n",
            "\n",
            "Aanast wyth ony me wanat'treaurhe:\n",
            "OUCathes t s, t bnoureethr the d mere d bethilthe gofthome tlor ouacather sorge ow hisesplervins toly strthoknenr t Fnebo theshondiabp dIshas\n",
            "Wtej f;etuturura e:\n",
            "CNI thon? u y.\n",
            "AU\n",
            "Whomuleeonthit wle otegcurdb'tary, Pt ce wowNwer p e r ld t in 'ency; mROnghat  thurutus per me y.\n",
            "Plloue denansel its bled mbpuakng th-ild mespurdn\n",
            "J atatur sthenfertshI NLYUS:\n",
            "F medeashalthiteor hakn wer h ous::UT:im'tthedlarmiso tatin agalan\n",
            "HIOCIFitheroui'venthelfdpumunove an\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1, 53,  ...,  1, 39, 52]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "model.generate(context, max_new_tokens=1200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/gp1-shakespeare.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-JBamXX6Ptv",
        "outputId": "048d7244-f192-48c5-bb02-accfb4ecc955"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fcvg5aoO5UUk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ntts-duI6CFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}